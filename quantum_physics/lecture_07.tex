\section{Lecture 7 (March 26th)}
\begin{rmk}
We have previously learned the Heisenberg uncertainty principle, expressible as
\[[\hat{x},\hat{p}]=i\hbar\;\mathrm{Id}\]
This is called Heisenberg-Weyl algebra. The commutator satisfies the first Bianchi identity. By mathematical induction, we can quite easily find that
\[[x,p^{n}]=ni\hbar p^{n-1}\]
\end{rmk}
\vspace{2ex}
\begin{recall}
We have seen how the expected value for the momentum operator was
\[\langle \hat{p}\rangle =\int dp\;\phi ^{*}(p)p\phi (p)\]
What would happen if we would try to find the expected value of the position using the momentum wave function?
\begin{align*}
\langle \hat{x}\rangle =&\int dx\;\mathrm{\Psi} ^{*}(x)x\mathrm{\Psi}(x)\\
=&\int dx\;\Big(\int \dfrac{dp'}{\sqrt{2\pi \hbar }}\phi ^{*}(p)\exp \Big(-\dfrac{ipi'x}{\hbar }\Big)\Big)x\Big(\int \dfrac{dp}{\sqrt{2\pi \hbar }}\phi (p)\exp \Big(\dfrac{ipx}{\hbar }\Big)\Big)\\
	=&-i\hbar \int dp\;dp'\;\phi ^{*}(p')\phi (p)\Big(\dfrac{\partial }{\partial p}\delta (p-p') \Big)\\
	=&-i\hbar \int dp'\;\phi ^{*}(p)\phi (p)\delta (p-p')\big|^{\infty }_{-\infty }+i\hbar \int dp'\;dp\; \phi ^{*}(p)\dfrac{\partial }{\partial p}(\phi (p)\delta (p-p'))
	=&\int dp\;\phi^{*}(p)\Big[i\hbar \dfrac{\partial }{\partial p} \Big]\phi (p)
\end{align*}
as
	\[\int dx\;x\exp \Big(\dfrac{i(p-p')x}{\hbar }\Big)=-i\hbar \dfrac{\partial }{\partial p}\int dx\;\exp \Big(\dfrac{i(p-p')x}{\hbar }\Big)=-i\hbar \dfrac{\partial }{\partial p}(2\pi \hbar \delta (p-p'))  \]
\end{recall}
\vspace{2ex}
\begin{defi}
The basis of a vector space ($V$) is defined as a linearly independent and spanning subset of $V$. By spanning, we mean that an arbitrary vector can be expressed as a linear combination of the subset.
\[v=\sum _{n}c_{n}v_{n}\]
By linearly independent, we mean that if 
\[\sum _{n}c_{n}v_{n}=0\]
	this implies $c_{n}=0$ for all $n$. By a finite dimensional vector space, we mean that the number of elements in a basis is finite. Meanwhile, an inner product of a complex vector space $V$ is a machine that takes in two vectors to create a scalar. There are multiple notations you can use for this inner product ($\langle v,w\rangle $ or $(v,w)$ to name a few). We require that the map satisfies
\begin{itemize}
	\item[(i)] $\langle v,v\rangle \geq 0$ and $\langle v,v\rangle =0$ if and only if $v=0$ (non-degeneracy)
	\item[(ii)] $\langle v,a_1w_1+a_2w_2\rangle =a_1\langle v,w_1\rangle+a_2\langle v,w_2\rangle  $ (linearity in the second argument)
	\item[(iii)] $\langle v,w\rangle ^{*}=\langle w|v\rangle $ (conjugate symmetry)
\end{itemize}
\end{defi}
\vspace{2ex}
\begin{ex}
An example is the $n$-dimensional complex space ${\bm C}^{n}$. An inner product of two vectors $v=(v_1,\ldots ,v_{n})$ and $w=(w_1,\ldots ,w_{n})$ is
\[\langle w,v\rangle =\sum ^{n}_{i=1}w_{i}^{*}v_{i}\]
This is called the standard inner product. 
\end{ex}
\vspace{2ex}
\begin{ex}
The $L^{2}(I)$ space is a space with square integrable functions on $I$ ($\int _{I}|f|^2<\infty $). The inner product of two vectors $f$ and $g$ is
\[\langle f,g\rangle =\int_{I} dx\;f^{*}g\]
This is an important inner product used for wave functions.
\end{ex}
\vspace{2ex}
\begin{ex}
The space of $n\times n$ complex matrices $M_{n\times n}({\bm C})$ has the following inner product.
\[\langle M_1,M_2\rangle =\mathrm{Tr}(M_1^{\dagger}M_2)\]
\end{ex}
\vspace{2ex}
\begin{defi}
A linear operator (or transformation) is a map $L:V\rightarrow W$ which satisfies
\[L(a_1v_1+a_2v_2)=a_1L(v_1)+a_2L(v_2)\]
Some examples are matrix multiplication, scalar multiplication, and derivation. Along this line, the Hermitian operator $\hat{H}$ is a linear operator. The reason why we want linear transformations is because it preserves superposition. If the input if a superposed state, the output is likewise.
\end{defi}
\vspace{2ex}
\begin{rmk}
We can represent any linear operator as a matrix (which is called the matrix representation of the operator). For simplicity, assume a finite dimensional vector space and we have 
\[L(v=\sum ^{n}_{i=1}c_{i}v_{i})=\sum ^{n}_{i=1}c_{i}L(v_{i})\]
For a single component $v_{i}$, we have
\[L(v_{i})=\sum ^{m}_{j=1}w_{j}L_{ji}\]
be aware of the order of the $j$ and $i$ for the matrix $L$. 
\end{rmk}
\vspace{2ex}

